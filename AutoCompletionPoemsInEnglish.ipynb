{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjRRrItIL-1B",
        "outputId": "ce1d3e5d-d6a6-4524-f405-0986f50c88c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Read the dataset from the file\n",
        "with open('data.txt', 'r') as file:\n",
        "    poems = file.readlines()\n",
        "\n",
        "# Tokenize the poems\n",
        "tokens = []\n",
        "for poem in poems:\n",
        "    # Preprocess the text by removing special characters and symbols\n",
        "    processed_poem = poem.lower().strip().replace(\".\", \"\")\n",
        "    tokens += processed_poem.split()\n",
        "\n",
        "# Create a vocabulary\n",
        "vocab = list(set(tokens))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "R5ucCtuHMAay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the window size\n",
        "window_size = 5\n",
        "\n",
        "# Generate training examples\n",
        "input_seqs = []\n",
        "target_seqs = []\n",
        "for i in range(len(tokens) - window_size):\n",
        "    input_seq = tokens[i:i+window_size]\n",
        "    target_seq = tokens[i+window_size]\n",
        "    input_seqs.append(input_seq)\n",
        "    target_seqs.append(target_seq)\n",
        "\n",
        "# Handle the last sequence that is shorter than the window size\n",
        "if len(tokens) >= window_size:\n",
        "    input_seq = tokens[-window_size:]\n",
        "    target_seq = tokens[-1]\n",
        "    input_seqs.append(input_seq)\n",
        "    target_seqs.append(target_seq)\n",
        "\n",
        "# Convert sequences to tensors\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "for input_seq, target_seq in zip(input_seqs, target_seqs):\n",
        "    input_tensors.append(torch.tensor([word_to_idx[word] for word in input_seq], dtype=torch.long))\n",
        "    target_tensors.append(torch.tensor(word_to_idx[target_seq], dtype=torch.long))"
      ],
      "metadata": {
        "id": "qovtt1q6MLUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutocompleteModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(AutocompleteModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "F7VjR2yAMOH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the hyperparameters\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create the model\n",
        "model = AutocompleteModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_ratio = 0.9\n",
        "train_size = int(train_ratio * len(input_tensors))\n",
        "train_inputs, val_inputs = input_tensors[:train_size], input_tensors[train_size:]\n",
        "train_targets, val_targets = target_tensors[:train_size], target_tensors[train_size:]\n",
        "\n",
        "# Train the model\n",
        "best_val_loss = float('inf')\n",
        "best_model_state_dict = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, len(train_inputs), batch_size):\n",
        "        batch_inputs = train_inputs[i:i+batch_size]\n",
        "        batch_targets = train_targets[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(torch.stack(batch_inputs))\n",
        "        loss = criterion(outputs, torch.stack(batch_targets))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_inputs)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_inputs), batch_size):\n",
        "            batch_inputs = val_inputs[i:i+batch_size]\n",
        "            batch_targets = val_targets[i:i+batch_size]\n",
        "\n",
        "            outputs = model(torch.stack(batch_inputs))\n",
        "            loss = criterion(outputs, torch.stack(batch_targets))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_inputs)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_state_dict = model.state_dict()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Load the best model state\n",
        "model.load_state_dict(best_model_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkS836NXMQc1",
        "outputId": "184b8cb3-e029-44f2-e82a-780c5b0a06d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100, Train Loss: 0.1014, Val Loss: 0.1030\n",
            "Epoch: 2/100, Train Loss: 0.0836, Val Loss: 0.1032\n",
            "Epoch: 3/100, Train Loss: 0.0748, Val Loss: 0.1043\n",
            "Epoch: 4/100, Train Loss: 0.0659, Val Loss: 0.1054\n",
            "Epoch: 5/100, Train Loss: 0.0571, Val Loss: 0.1065\n",
            "Epoch: 6/100, Train Loss: 0.0479, Val Loss: 0.1080\n",
            "Epoch: 7/100, Train Loss: 0.0392, Val Loss: 0.1100\n",
            "Epoch: 8/100, Train Loss: 0.0314, Val Loss: 0.1142\n",
            "Epoch: 9/100, Train Loss: 0.0247, Val Loss: 0.1152\n",
            "Epoch: 10/100, Train Loss: 0.0192, Val Loss: 0.1151\n",
            "Epoch: 11/100, Train Loss: 0.0149, Val Loss: 0.1168\n",
            "Epoch: 12/100, Train Loss: 0.0114, Val Loss: 0.1191\n",
            "Epoch: 13/100, Train Loss: 0.0088, Val Loss: 0.1211\n",
            "Epoch: 14/100, Train Loss: 0.0067, Val Loss: 0.1234\n",
            "Epoch: 15/100, Train Loss: 0.0052, Val Loss: 0.1259\n",
            "Epoch: 16/100, Train Loss: 0.0041, Val Loss: 0.1283\n",
            "Epoch: 17/100, Train Loss: 0.0032, Val Loss: 0.1301\n",
            "Epoch: 18/100, Train Loss: 0.0027, Val Loss: 0.1314\n",
            "Epoch: 19/100, Train Loss: 0.0022, Val Loss: 0.1328\n",
            "Epoch: 20/100, Train Loss: 0.0019, Val Loss: 0.1336\n",
            "Epoch: 21/100, Train Loss: 0.0017, Val Loss: 0.1342\n",
            "Epoch: 22/100, Train Loss: 0.0015, Val Loss: 0.1350\n",
            "Epoch: 23/100, Train Loss: 0.0014, Val Loss: 0.1363\n",
            "Epoch: 24/100, Train Loss: 0.0014, Val Loss: 0.1377\n",
            "Epoch: 25/100, Train Loss: 0.0013, Val Loss: 0.1389\n",
            "Epoch: 26/100, Train Loss: 0.0013, Val Loss: 0.1407\n",
            "Epoch: 27/100, Train Loss: 0.0012, Val Loss: 0.1419\n",
            "Epoch: 28/100, Train Loss: 0.0012, Val Loss: 0.1428\n",
            "Epoch: 29/100, Train Loss: 0.0012, Val Loss: 0.1432\n",
            "Epoch: 30/100, Train Loss: 0.0011, Val Loss: 0.1444\n",
            "Epoch: 31/100, Train Loss: 0.0011, Val Loss: 0.1441\n",
            "Epoch: 32/100, Train Loss: 0.0011, Val Loss: 0.1441\n",
            "Epoch: 33/100, Train Loss: 0.0011, Val Loss: 0.1450\n",
            "Epoch: 34/100, Train Loss: 0.0010, Val Loss: 0.1456\n",
            "Epoch: 35/100, Train Loss: 0.0010, Val Loss: 0.1461\n",
            "Epoch: 36/100, Train Loss: 0.0010, Val Loss: 0.1472\n",
            "Epoch: 37/100, Train Loss: 0.0010, Val Loss: 0.1477\n",
            "Epoch: 38/100, Train Loss: 0.0010, Val Loss: 0.1470\n",
            "Epoch: 39/100, Train Loss: 0.0009, Val Loss: 0.1487\n",
            "Epoch: 40/100, Train Loss: 0.0009, Val Loss: 0.1476\n",
            "Epoch: 41/100, Train Loss: 0.0010, Val Loss: 0.1488\n",
            "Epoch: 42/100, Train Loss: 0.0009, Val Loss: 0.1505\n",
            "Epoch: 43/100, Train Loss: 0.0009, Val Loss: 0.1522\n",
            "Epoch: 44/100, Train Loss: 0.0009, Val Loss: 0.1523\n",
            "Epoch: 45/100, Train Loss: 0.0008, Val Loss: 0.1505\n",
            "Epoch: 46/100, Train Loss: 0.0008, Val Loss: 0.1509\n",
            "Epoch: 47/100, Train Loss: 0.0008, Val Loss: 0.1527\n",
            "Epoch: 48/100, Train Loss: 0.0008, Val Loss: 0.1524\n",
            "Epoch: 49/100, Train Loss: 0.0008, Val Loss: 0.1525\n",
            "Epoch: 50/100, Train Loss: 0.0007, Val Loss: 0.1530\n",
            "Epoch: 51/100, Train Loss: 0.0007, Val Loss: 0.1531\n",
            "Epoch: 52/100, Train Loss: 0.0007, Val Loss: 0.1539\n",
            "Epoch: 53/100, Train Loss: 0.0007, Val Loss: 0.1539\n",
            "Epoch: 54/100, Train Loss: 0.0007, Val Loss: 0.1538\n",
            "Epoch: 55/100, Train Loss: 0.0007, Val Loss: 0.1539\n",
            "Epoch: 56/100, Train Loss: 0.0007, Val Loss: 0.1543\n",
            "Epoch: 57/100, Train Loss: 0.0007, Val Loss: 0.1557\n",
            "Epoch: 58/100, Train Loss: 0.0007, Val Loss: 0.1551\n",
            "Epoch: 59/100, Train Loss: 0.0007, Val Loss: 0.1562\n",
            "Epoch: 60/100, Train Loss: 0.0007, Val Loss: 0.1546\n",
            "Epoch: 61/100, Train Loss: 0.0007, Val Loss: 0.1538\n",
            "Epoch: 62/100, Train Loss: 0.0008, Val Loss: 0.1554\n",
            "Epoch: 63/100, Train Loss: 0.0008, Val Loss: 0.1561\n",
            "Epoch: 64/100, Train Loss: 0.0008, Val Loss: 0.1576\n",
            "Epoch: 65/100, Train Loss: 0.0007, Val Loss: 0.1586\n",
            "Epoch: 66/100, Train Loss: 0.0007, Val Loss: 0.1582\n",
            "Epoch: 67/100, Train Loss: 0.0007, Val Loss: 0.1568\n",
            "Epoch: 68/100, Train Loss: 0.0007, Val Loss: 0.1583\n",
            "Epoch: 69/100, Train Loss: 0.0007, Val Loss: 0.1599\n",
            "Epoch: 70/100, Train Loss: 0.0007, Val Loss: 0.1595\n",
            "Epoch: 71/100, Train Loss: 0.0007, Val Loss: 0.1600\n",
            "Epoch: 72/100, Train Loss: 0.0007, Val Loss: 0.1597\n",
            "Epoch: 73/100, Train Loss: 0.0007, Val Loss: 0.1602\n",
            "Epoch: 74/100, Train Loss: 0.0007, Val Loss: 0.1610\n",
            "Epoch: 75/100, Train Loss: 0.0007, Val Loss: 0.1611\n",
            "Epoch: 76/100, Train Loss: 0.0006, Val Loss: 0.1617\n",
            "Epoch: 77/100, Train Loss: 0.0006, Val Loss: 0.1603\n",
            "Epoch: 78/100, Train Loss: 0.0006, Val Loss: 0.1622\n",
            "Epoch: 79/100, Train Loss: 0.0006, Val Loss: 0.1619\n",
            "Epoch: 80/100, Train Loss: 0.0006, Val Loss: 0.1637\n",
            "Epoch: 81/100, Train Loss: 0.0006, Val Loss: 0.1635\n",
            "Epoch: 82/100, Train Loss: 0.0006, Val Loss: 0.1625\n",
            "Epoch: 83/100, Train Loss: 0.0006, Val Loss: 0.1617\n",
            "Epoch: 84/100, Train Loss: 0.0006, Val Loss: 0.1624\n",
            "Epoch: 85/100, Train Loss: 0.0006, Val Loss: 0.1648\n",
            "Epoch: 86/100, Train Loss: 0.0007, Val Loss: 0.1647\n",
            "Epoch: 87/100, Train Loss: 0.0023, Val Loss: 0.1533\n",
            "Epoch: 88/100, Train Loss: 0.0024, Val Loss: 0.1583\n",
            "Epoch: 89/100, Train Loss: 0.0009, Val Loss: 0.1603\n",
            "Epoch: 90/100, Train Loss: 0.0007, Val Loss: 0.1609\n",
            "Epoch: 91/100, Train Loss: 0.0007, Val Loss: 0.1617\n",
            "Epoch: 92/100, Train Loss: 0.0007, Val Loss: 0.1623\n",
            "Epoch: 93/100, Train Loss: 0.0006, Val Loss: 0.1628\n",
            "Epoch: 94/100, Train Loss: 0.0006, Val Loss: 0.1633\n",
            "Epoch: 95/100, Train Loss: 0.0006, Val Loss: 0.1637\n",
            "Epoch: 96/100, Train Loss: 0.0006, Val Loss: 0.1642\n",
            "Epoch: 97/100, Train Loss: 0.0006, Val Loss: 0.1646\n",
            "Epoch: 98/100, Train Loss: 0.0006, Val Loss: 0.1649\n",
            "Epoch: 99/100, Train Loss: 0.0006, Val Loss: 0.1653\n",
            "Epoch: 100/100, Train Loss: 0.0006, Val Loss: 0.1657\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "maxLengthForm = 7 #@param {type:\"integer\"}\n",
        "seed = 2348 #@param {type:\"slider\", min:1, max:2500, step:1}\n",
        "suggestFor = \"my heart\" #@param {type:\"string\"}\n",
        "# Set the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate autocompletions\n",
        "input_sequence = suggestFor.lower()\n",
        "max_length = maxLengthForm\n",
        "beam_width = seed\n",
        "\n",
        "\n",
        "def score_beam_candidates(beam_candidates):\n",
        "    scores = []\n",
        "    for candidate in beam_candidates:\n",
        "        candidate_tensor = torch.tensor([word_to_idx[word] for word in candidate], dtype=torch.long).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = model(candidate_tensor)\n",
        "            score = torch.log_softmax(output, dim=1).sum()\n",
        "        scores.append(score.item())\n",
        "    return torch.tensor(scores)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Tokenize the input sequence\n",
        "    input_tokens = input_sequence.lower().split()\n",
        "\n",
        "    # Filter out words that are not in the vocabulary\n",
        "    input_tokens = [token for token in input_tokens if token in vocab]\n",
        "\n",
        "    # Check if the input sequence is empty after filtering\n",
        "    if len(input_tokens) == 0:\n",
        "        print(\"No valid words in the input sequence. Please try again with valid words.\")\n",
        "        exit()\n",
        "\n",
        "    input_tensor = torch.tensor([word_to_idx[word] for word in input_tokens], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Generate autocompletions using beam search\n",
        "    output_sequence = input_tokens[:]\n",
        "    for _ in range(max_length):\n",
        "        output = model(input_tensor)\n",
        "        _, topk_indices = torch.topk(output, beam_width, dim=1)\n",
        "\n",
        "        beam_candidates = []\n",
        "        for idx in topk_indices[0]:\n",
        "            predicted_word = idx_to_word[idx.item()]\n",
        "            beam_candidates.append(output_sequence + [predicted_word])\n",
        "\n",
        "        scores = score_beam_candidates(beam_candidates)\n",
        "        topk_scores, topk_indices = torch.topk(scores, beam_width)\n",
        "\n",
        "        output_sequence = beam_candidates[topk_indices[0].item()]\n",
        "        input_tensor = torch.tensor([word_to_idx[word] for word in output_sequence], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    autocompletion = ' '.join(output_sequence)\n",
        "    print(f\"Suggestion: {autocompletion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcaE5ofjMUhF",
        "outputId": "064b74bf-dce7-4193-fd64-52c8d862044c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggestion: my heart letting roaming autumn harmonious tree, human forest's\n"
          ]
        }
      ]
    }
  ]
}